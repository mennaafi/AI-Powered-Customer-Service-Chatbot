{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv \n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "model_name = os.getenv(\"MODEL_NAME\", \"gemini-1.5-flash\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chatbot_response(messages):\n",
    "    input_messages = []\n",
    "    for message in messages:\n",
    "        input_messages.append({\"role\": message[\"role\"], \"parts\": message[\"parts\"]})\n",
    "    response = model.generate_content(messages)  \n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"parts\": [\n",
      "              {\n",
      "                \"text\": \"Rome\\n\"\n",
      "              }\n",
      "            ],\n",
      "            \"role\": \"model\"\n",
      "          },\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"avg_logprobs\": -0.003409839700907469\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 9,\n",
      "        \"candidates_token_count\": 2,\n",
      "        \"total_token_count\": 11\n",
      "      }\n",
      "    }),\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "messages = [{'role':'user','parts':\"What's the capital of Italy?\"}]\n",
    "response = get_chatbot_response(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "            You are a helpful AI assistant for a coffee shop application which serves drinks and pastries.\n",
    "            Your task is to determine whether the user is asking something relevant to the coffee shop or not.\n",
    "            The user is allowed to:\n",
    "            1. Ask questions about the coffee shop, like location, working hours, menue items and coffee shop related questions.\n",
    "            2. Ask questions about menue items, they can ask for ingredients in an item and more details about the item.\n",
    "            3. Make an order.\n",
    "            4. ASk about recommendations of what to buy.\n",
    "\n",
    "            The user is NOT allowed to:\n",
    "            1. Ask questions about anything else other than our coffee shop.\n",
    "            2. Ask questions about the staff or how to make a certain menue item.\n",
    "\n",
    "            Your output should be in a structured json format like so. each key is a string and each value is a string. Make sure to follow the format exactly:\n",
    "            {\n",
    "            \"chain of thought\": go over each of the points above and make see if the message lies under this point or not. Then you write some your thoughts about what point is this input relevant to.\n",
    "            \"decision\": \"allowed\" or \"not allowed\". Pick one of those. and only write the word.\n",
    "            \"message\": leave the message empty if it's allowed, otherwise write \"Sorry, I can't help with that. Can I help you with your order?\"\n",
    "            }\n",
    "            \"\"\"\n",
    "\n",
    "prompt = input(\"User: \")\n",
    "messages.append({\"role\": \"user\", \"parts\": prompt})\n",
    "\n",
    "\n",
    "input_messages = [{\"role\": \"model\", \"parts\": system_prompt}] + messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt =  \"\"\"\n",
    "            You are a helpful AI assistant for a coffee shop application.\n",
    "            Your task is to determine what agent should handle the user input. You have 3 agents to choose from:\n",
    "            1. details_agent: This agent is responsible for answering questions about the coffee shop, like location, delivery places, working hours, details about menue items. Or listing items in the menu items. Or by asking what we have.\n",
    "            2. order_taking_agent: This agent is responsible for taking orders from the user. It's responsible to have a conversation with the user about the order untill it's complete.\n",
    "            3. recommendation_agent: This agent is responsible for giving recommendations to the user about what to buy. If the user asks for a recommendation, this agent should be used.\n",
    "\n",
    "            Your output should be in a structured json format like so. each key is a string and each value is a string. Make sure to follow the format exactly:\n",
    "            {\n",
    "            \"chain of thought\": go over each of the agents above and write some your thoughts about what agent is this input relevant to.\n",
    "            \"decision\": \"details_agent\" or \"order_taking_agent\" or \"recommendation_agent\". Pick one of those. and only write the word.\n",
    "            \"message\": leave the message empty.\n",
    "            }\n",
    "            \"\"\"\n",
    "prompt = input(\"User: \")\n",
    "messages.append({\"role\": \"user\", \"parts\": prompt})\n",
    "\n",
    "\n",
    "input_messages = [{\"role\": \"model\", \"parts\": system_prompt}] + messages   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n  \"chain of thought\": \"The user is asking a question that is unrelated to the coffee shop\\'s offerings, location, hours, or menu.  None of the agents are equipped to answer this question. However, since it\\'s not a question about ordering or a request for recommendations, the details agent is the least inappropriate choice.\",\\n  \"decision\": \"details_agent\",\\n  \"message\": \"\"\\n}\\n```\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_output = get_chatbot_response(input_messages).text\n",
    "chatbot_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(output):\n",
    "        cleaned_output = \"\\n\".join(output.splitlines()[1:]).strip('`')\n",
    "        output = json.loads(cleaned_output)\n",
    "\n",
    "        dict_output = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"parts\": output['message'],\n",
    "            \"memory\": {\"agent\":\"classification_agent\",\n",
    "                       \"classification_decision\": output['decision']\n",
    "                      }\n",
    "        }\n",
    "        return dict_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'parts': '',\n",
       " 'memory': {'agent': 'classification_agent',\n",
       "  'classification_decision': 'details_agent'}}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = postprocess(chatbot_output)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'details_agent'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['memory']['classification_decision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(output):\n",
    "        cleaned_output = \"\\n\".join(output.splitlines()[1:]).strip('`')\n",
    "        output = json.loads(cleaned_output)\n",
    "\n",
    "        dict_output = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"parts\": output['message'],\n",
    "            \"memory\": {\"agent\":\"guard_agent\",\n",
    "                       \"guard_decision\": output['decision']\n",
    "                      }\n",
    "        }\n",
    "        return dict_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'parts': \"Sorry, I can't help with that. Can I help you with your order?\",\n",
       " 'memory': {'agent': 'guard_agent', 'guard_decision': 'not allowed'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = postprocess(chatbot_output)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "okay\n"
     ]
    }
   ],
   "source": [
    "if output[\"memory\"][\"guard_decision\"] == \"not allowed\":\n",
    "    print(\"not allowed\")\n",
    "else:\n",
    "    print(\"alowed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chatbot_response(system_prompt, user_messages):\n",
    "    model = genai.GenerativeModel(model_name)\n",
    "    \n",
    "    chat = model.start_chat(history=[{\"role\": \"model\", \"parts\": [{\"text\": system_prompt}]}])\n",
    "    \n",
    "    for message in user_messages[-3:]:  \n",
    "        user_message = {\"role\": \"user\", \"parts\": [{\"text\": message}]}\n",
    "        response = chat.send_message(user_message)\n",
    "        \n",
    "\n",
    "    return chat.history[-1].parts[0].text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A computer works by executing a series of instructions, called a program, written in a language it understands.  These instructions manipulate data stored in its memory and on storage devices, performing calculations, manipulating images and text, and interacting with external devices like keyboards, screens, and the internet, all based on the logic defined by the program.\\n'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_messages = [\n",
    "    \"In one sentence, explain how a computer works to a young child.\",\n",
    "    \"Okay, how about a more detailed explanation to a high schooler?\"\n",
    "]\n",
    "\n",
    "response = get_chatbot_response(user_messages)\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ramen is a Japanese noodle soup.  It consists of:\\n\\n* **Broth:**  This is the base of the dish and can be made from a variety of ingredients, most commonly pork, chicken, beef, seafood, or combinations thereof.  Often it includes dashi (a stock made from kombu kelp and bonito flakes), soy sauce, and mirin (sweet rice wine).  The broth is what gives ramen its distinct flavor.\\n\\n* **Noodles:**  These are typically wheat noodles, but other types can sometimes be used. They are usually thin and slightly alkaline, giving them a characteristic chewiness.\\n\\n* **Toppings:**  This is where the immense variety of ramen comes in. Common toppings include:\\n    * Chashu (braised pork belly)\\n    * Soft-boiled eggs (often marinated in soy sauce and mirin)\\n    * Nori (dried seaweed sheets)\\n    * Menma (fermented bamboo shoots)\\n    * Scallions (green onions)\\n    * Bean sprouts\\n    * Corn\\n    * Mushrooms\\n    * Spiced oil\\n\\n\\nWhile instant ramen is a popular and widely available quick meal, authentic ramen is a much more complex and carefully crafted dish.  The type of broth, noodles, and toppings used create a wide range of styles and regional variations.\\n'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prompt = [\"what is ramen\"]\n",
    "get_chatbot_response(user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chatbot_response(messages):\n",
    "    response = model.generate_content(messages)  \n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"parts\": [\n",
      "              {\n",
      "                \"text\": \"The capital of Italy is Rome.\\n\"\n",
      "              }\n",
      "            ],\n",
      "            \"role\": \"model\"\n",
      "          },\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"avg_logprobs\": -0.00046353484503924847\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 15,\n",
      "        \"candidates_token_count\": 8,\n",
      "        \"total_token_count\": 23\n",
      "      }\n",
      "    }),\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are a helpful assistant.\"  \n",
    "messages = [{'text': system_prompt}]  \n",
    "messages.append({'text': \"What's the capital of Italy?\"}) \n",
    "\n",
    "response = get_chatbot_response(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three techniques of prompt engineering:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) structured output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "[\n",
      "  {\n",
      "    \"country\": \"France\",\n",
      "    \"capital\": \"Paris\"\n",
      "  }\n",
      "]\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant that answer questions about capitals of countries.\n",
    "\n",
    "Your output should be in a structured json format exactly like the one bellow. You are not allowed to write anything other than the json object:\n",
    "[\n",
    "{\n",
    "    country: the country that you will get the capital of \n",
    "    capital: the capital of the country stated\n",
    "}\n",
    "]\n",
    "\"\"\"\n",
    "messages = [{'text': system_prompt}]  \n",
    "response = get_chatbot_response(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) structured input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Italy - Rome\n",
      "2. France - Paris\n",
      "3. Germany - Berlin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"\n",
    "Get me the capitals of the following countries:\n",
    "```\n",
    "1. Italy\n",
    "2. France\n",
    "3. Germany\n",
    "``\n",
    "\"\"\"\n",
    "messages = [{'text': system_prompt}]  \n",
    "response = get_chatbot_response(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3)  Chain of thought : Give model time to think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"steps\": \"Following BEDMAS (Brackets, Exponents, Division and Multiplication from left to right, Addition and Subtraction from left to right):\\n\\n1. Division: 259 / 2 = 129.5\\n2. Multiplication (1): 129.5 * 8654 = 1120000 + 5190 + 43270 + 7740 + 1295 = 1122363\\n3. Multiplication (2): 91072 * 33 = 2997376 \\n4. Addition: 1122363 + 2997376 = 4119739\\n5. Subtraction: 4119739 - 12971 = 4106768\",\n",
      "  \"result\": 4106768\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"\n",
    "Calculate the result of this equation: 259/2*8654+91072*33-12971\n",
    "\n",
    "Your output should be in a structured json format exactly like the one bellow. You are not allowed to write anything other than the json object:\n",
    "{\n",
    "    steps: This is where you solve the equation bit by bit following the BEDMAS order of operations. You need to show your work and calculate each step leading to final result. Feel free to write here in free text. \n",
    "    result: The final number resulted from calculating the equation above\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "messages = [{'text': system_prompt}]  \n",
    "response = get_chatbot_response(messages)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  RAG - Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_name = os.getenv(\"EMBEDDING_MODEL_NAME\", \"models/text-embedding-004\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text_input):\n",
    "    result = genai.embed_content(\n",
    "        model=embedding_model_name , \n",
    "        content=text_input\n",
    "    )\n",
    "\n",
    "    embedding = result['embedding']\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "iphone_16 = \"\"\"\n",
    "The iPhone 16 introduces several exciting updates, making it one of Apple's most advanced smartphones to date. It features a larger 6.1-inch display for the base model and a 6.7-inch screen for the iPhone 16 Plus, with thinner bezels and a more durable Ceramic Shield. The iPhone 16 Pro and Pro Max boast even larger displays, measuring 6.3 and 6.9 inches respectively, offering the thinnest bezels seen on any Apple product so far.\n",
    "\n",
    "Powered by the new A18 chip (A18 Pro for the Pro models), these phones deliver significant performance improvements, with enhanced neural engine capabilities, faster GPU for gaming, and machine learning tasks. The camera systems are also upgraded, with the base iPhone 16 sporting a dual-camera setup with a 48MP main sensor. The Pro models offer a 48MP Ultra Wide and 5x telephoto camera, enhanced by Apple’s \"Camera Control\" button for more flexible photography options.\n",
    "\n",
    "Apple also introduced advanced audio features like \"Audio Mix,\" which uses machine learning to separate background sounds from speech, allowing for more refined audio capture during video recording. Battery life has been extended, especially in the iPhone 16 Pro Max, which is claimed to have the longest-lasting battery of any iPhone \n",
    "9TO5MAC\n",
    "\n",
    "APPLEMAGAZINE\n",
    ".\n",
    "\n",
    "Additionally, Apple has switched to USB-C for faster charging and data transfer, and the Pro models now support up to 2x faster video encoding. The starting prices remain consistent with previous generations, with the iPhone 16 starting at $799, while the Pro models start at $999\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "samsung_s23 = \"\"\"\n",
    "The Samsung Galaxy S23 brings some incremental but notable upgrades to its predecessor, the Galaxy S22. It features the Snapdragon 8 Gen 2 processor, a powerful chip optimized for the S23 series, delivering enhanced performance, especially for gaming and multitasking. This chip ensures top-tier speed and efficiency across all models, from the base S23 to the larger S23+ and S23 Ultra​\n",
    "STUFF\n",
    "\n",
    "TECHRADAR\n",
    ".\n",
    "\n",
    "In terms of design, the S23's camera module has been streamlined by removing the raised metal contour around the cameras, creating a cleaner, sleeker look. It also sports the same 6.1-inch 120Hz AMOLED display, protected by tougher Gorilla Glass Victus 2, making it more resistant to scratches and drops​\n",
    "TECHRADAR\n",
    ".\n",
    "\n",
    "The S23 Ultra stands out with its 200MP main camera, offering impressive photo clarity, especially in low-light conditions. The selfie camera across the series has been updated to a 12MP sensor, resulting in sharper selfies. The Ultra model also includes productivity tools such as the S-Pen, which remains an essential feature for note-taking and creative tasks​\n",
    "STUFF\n",
    "\n",
    "TECHRADAR\n",
    ".\n",
    "\n",
    "Battery life is solid, with the S23 Ultra featuring a 5000mAh battery that lasts comfortably through a day of heavy use. However, charging speeds still lag behind some competitors, with 45W wired charging, which is slower than other brands offering up to 125W charging​\n",
    "STUFF\n",
    ".\n",
    "\n",
    "Overall, the Galaxy S23 series enhances performance, durability, and camera quality, making it a strong contender for users seeking a high-performance flagship.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [iphone_16,samsung_s23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0061438465, -0.001949737, -0.014181823, 0.009104185, 0.03299688, 0.06654366, 0.06271397, 0.05014754, -0.0008453271, 0.0136248255, -0.015688933, -0.0049665333, 0.023984237, 0.0128566725, 0.061009087, -0.061809275, 0.037997022, 0.03829026, -0.10877881, 0.0024127471, 0.027050512, -0.027518885, 0.0044580325, 0.01585691, -0.063779265, -0.010496415, -0.03348543, -0.0576879, 0.0561909, 0.009032659, 0.036307517, 0.008656795, -0.047088824, -0.06696679, 0.019250266, 0.03151103, -0.008380219, 0.022357618, 0.044863444, -0.013353064, -0.06429306, -0.01801903, 0.0059479866, 0.0168679, -0.017955363, -0.03773597, -0.010616519, -0.017767087, -0.020344868, 0.044458754, 0.050184574, -0.020537598, -0.0040852334, -0.040704187, -0.024916304, -0.0027411678, -0.04946308, -0.039144795, 0.032749783, -0.014032241, -0.033588644, -0.06989241, -0.012718956, -0.04035856, -0.014201617, 0.030976135, -0.038853675, -0.04340171, -0.005443529, 0.033408187, 0.013368484, 0.027578896, -0.040732045, -0.009081936, 0.028915474, 0.033279926, 0.00741034, -0.011847474, 0.030923724, 0.026494045, 0.026236394, -0.004783786, 0.06684517, 0.01191383, -0.014570825, 0.0076401555, 0.0014953048, -0.028627262, -0.044738524, 0.011014791, 0.07079518, 0.051425155, -0.0069199903, -0.05893263, 0.08980992, -0.034376543, -0.016141266, -0.06779282, -0.036727935, -0.0045520356, 0.0055569042, 0.029029775, -0.019001069, -0.04339495, -0.020090507, 0.025475364, -0.026233768, 0.007178302, -0.08834408, -0.0031112242, -0.021775456, 0.042530715, 0.025747798, -0.0041681845, -0.02414401, 0.029195005, 0.0023246307, -0.004982857, 0.033248845, 0.026547596, -0.0023216105, 0.025481692, 0.014965062, 0.023192037, 0.07713643, 0.032474604, 0.03628191, 0.02556828, -0.009290915, 0.020200439, 0.07752868, 0.004412904, 0.033595722, 0.030366564, -0.023574745, -0.014130638, 0.024299834, 0.059367277, 0.017149637, -0.0014790008, -0.0070503964, -0.023223035, -0.109840974, 0.011223977, -0.004298204, 0.006050948, -0.011455488, 0.03778409, -0.057935107, -0.012023521, -0.07664796, -0.036629323, -0.015781227, 0.10202691, -0.060546666, -0.025064928, 0.035713207, -0.001012294, 0.06164647, 0.041126937, 0.0063995724, -0.04944215, 0.010386644, 0.02556562, -0.03999377, -0.05725485, -0.003627668, -0.048316225, -0.01974863, 0.0013657531, -0.03619084, 0.023416946, -0.09531524, 7.310227e-05, 0.049666733, 0.0022159494, 0.022528682, -0.04099934, -0.02107068, 0.041563045, 0.084672086, -0.04547627, -0.02414598, -0.020337515, 0.014246706, -0.0010395251, -0.029251374, 0.037168413, 0.0058180764, 0.06271927, -0.008356847, 0.0064175725, 0.043355107, 0.046657063, -0.030241357, 0.058585763, -0.017780265, -0.056492124, 0.05277173, -0.041223463, 0.08377427, -0.041115284, -0.012220217, -0.05068845, -0.027559124, 0.031855118, -0.031537265, -0.033680525, -0.04101342, -0.029259942, 0.0036264595, -0.069190174, 0.014395987, -0.015765158, 0.03217664, 0.022065954, 0.072884046, -0.007222939, 0.07687833, 0.03011834, 0.008115437, -0.0074742856, -0.04301627, -0.011576418, 0.011845528, 0.03721785, -0.009026335, 0.017474059, -0.0034697475, -0.021598062, 0.056695133, -0.006887194, 0.00013149361, 0.054257903, -0.0587317, 0.04974173, 0.02771886, -0.04496951, -0.053886842, -0.034831867, 0.021618016, 0.0066389227, 0.012543594, 0.03523651, 0.02826285, -0.0061694393, 0.0050444347, 0.009716949, -0.040906493, -0.020296406, 0.016900426, 0.013423517, -0.0013505409, 0.053110804, -0.0073921727, 0.048374068, -0.03820398, -0.013339139, 0.036174256, 0.019154627, 0.032367535, -0.026464695, -0.025210407, -0.008445873, 0.008550903, -0.06031607, -0.010922194, 0.011691845, 0.0033200982, -0.030050516, 0.052580923, 0.005254173, -0.05552272, -0.005198769, 1.6318681e-05, 0.01766601, -0.04159139, 0.057434507, -0.034780312, -0.037357666, 0.045559637, 0.013491974, -0.009435851, -0.018864851, -0.00259893, 0.0005405687, -0.0227679, 0.038806256, -0.038916506, -0.06045805, 0.038861506, 0.06949179, 0.011414748, -0.0990065, -0.007757819, 0.04787572, 0.07172728, 0.016816543, -0.032111567, 0.0275845, 0.043177053, 0.039074294, -0.018942498, 0.016864788, -0.013666354, 0.0015793042, -0.03740752, 0.002006888, 0.017765773, -0.004551289, 0.01765234, 0.0061386423, -0.07738085, 0.021941563, -0.05478423, -0.018098807, -0.100489974, 0.0071120067, -0.020211348, 0.014678862, 0.032262903, 0.026559316, -0.047985546, 0.0061004777, 0.053017214, -0.013579071, 0.01857599, -0.014647708, 0.038210336, -0.043948248, 0.029860886, -0.03163119, 0.014352447, -0.036636513, 0.00873036, 0.019514227, -0.02921836, 0.01602969, 0.053577337, 0.014404568, -0.010860749, -0.00706946, 0.065708116, 0.033407442, 0.036195815, -0.024777189, -0.037281826, -0.046577115, -0.0025721255, -0.00010141574, -0.021467617, 0.0012420192, -0.022597665, 0.011581202, 0.005009312, 0.06266456, 0.03011741, -0.0048793633, 0.011450464, 0.04632435, -0.04269155, 0.04013105, -0.0042940276, 0.051815245, 0.016471723, -0.044651166, 0.080757774, 0.024175806, -0.0018804044, -0.031423252, 0.0032871133, 0.047365867, -0.018295886, 0.07051318, -0.006167894, -0.033061963, -0.009578214, 0.023019264, 0.03827152, -0.038825683, -0.042851865, -0.06158887, -0.0125585925, -0.013518098, -0.028739214, 0.04459597, -0.07372332, 0.005493004, 0.03448417, -0.03616753, -0.011312742, 0.038314495, 0.08333428, 0.008477575, -0.04926504, 0.050630253, 0.017891163, -0.014800008, -0.005695452, 0.066367775, -6.9522735e-05, -0.032723565, -0.02716881, -0.018452248, 0.004366098, -0.0059189694, 0.08923223, -0.016253559, 0.005298713, -0.032238413, 0.011822261, -0.021261506, -0.03819657, -0.02490688, 0.017336352, -0.04610897, -0.078943096, -0.012673272, -0.017357038, 0.08108579, -0.019292576, -0.004216888, -0.007462119, -0.06516049, -0.014220273, -0.03926146, 0.011521712, -0.04010816, -0.0038667452, 0.0043969066, 0.023101445, -5.3220643e-05, -0.045764834, 0.0008675962, 0.027985534, 0.05752446, -0.009312771, -0.014179943, -0.017157374, 0.039694484, 0.019023867, 0.0065255715, 0.014861925, 0.029628733, 0.0012131586, 0.011696849, 0.019977685, -0.02784227, -0.030437933, -0.045025297, 0.026450241, -0.05192561, -0.014763391, -0.009509817, -0.074120685, 0.081816405, -0.02258959, -8.402091e-05, -0.028620616, 0.06482518, -0.004057908, -0.033997454, 0.042498734, -0.0060118968, 0.05458341, 0.012139667, -0.018336035, 0.025497274, 0.024769934, -0.0017438426, 0.03422619, 0.010688108, 0.012535863, -0.016456097, -0.0016478803, 0.06194357, 0.04643058, -0.02618735, -0.043376558, -0.021372857, 0.0032020793, 0.013162401, -0.028835583, -0.002817683, 0.027019642, -0.015132183, -0.018246125, -0.0074710897, -0.03362544, -0.0070982086, 0.034611873, 0.034881998, -0.010230675, -0.015449984, -0.02569208, -0.07437357, 0.029888647, 0.00068194367, -0.073341295, 0.06827071, 0.059884783, 0.03176521, 0.007365406, -0.05564285, -0.0068432363, 0.023881372, -0.01149914, 0.029762987, 0.037974745, -0.01729568, 0.06769709, -0.009708757, 0.01557392, -0.0011732846, -0.01894866, 0.04264181, 0.009663404, -0.0035295847, -0.008123267, 0.06267819, -0.043449707, -0.07222501, -0.0205447, 0.029411243, 0.00554554, 0.0668641, 0.021319674, 0.013435333, 0.034950398, -0.05036028, 0.027557135, 0.03452506, 0.017814081, 0.020530798, 0.004912502, 0.04528987, -0.00980418, 0.009385702, 0.0015745722, -0.008081523, -0.011534191, 0.03930442, -0.010825008, 0.027807478, 0.030143905, 0.038076147, -0.04344162, 0.018496454, 0.028576143, -0.007623292, 0.022700228, -0.004929047, -0.0665929, -0.0068867654, 0.020481082, -0.014880236, 0.01959751, 0.027249867, -0.0374286, -0.03151599, -0.011326056, 0.043834195, -0.012905719, -0.026997413, -0.055614714, -0.025056161, -0.06300782, 0.044542205, -0.0038028045, 0.019072529, -0.041993853, -0.052420218, -0.011401681, -0.065855905, 0.023301117, 0.011384619, -0.050694443, 0.00095458573, 0.054039523, -0.023574878, 0.0027065764, 0.030648505, -0.044435084, -0.028728426, -0.003591411, -0.022664003, 0.061966535, 0.018404417, 0.027841937, -0.059751667, 0.0057180747, 0.032302603, 0.00077673484, 0.014690562, 0.06506412, -0.002760937, -0.03139372, -0.014061716, 0.029735422, -0.0063536307, 0.054879654, -0.04568657, 0.032667175, 0.031029547, -0.030332658, -0.06917734, -0.0307617, -0.048788767, 0.0061171874, 0.0038209623, 0.042286173, 0.03862395, 0.013075962, 0.018271567, -0.06657784, 0.05148509, -0.009638538, 0.02104228, 0.0048432546, -0.048072226, 0.024716964, 0.016115908, 0.009044127, 0.05934813, -0.038535148, -0.015002752, -0.022222705, 0.019596094, 0.07924149, 0.039616745, -0.0066770427, -0.0021696195, 0.06657202, -0.042257205, -0.022451205, -0.01458152, 0.038196042, -0.0006323806, 0.021341145, -0.076872155, -0.027423104, 0.016918428, 0.08400359, -0.0146640325, -0.04325587, 0.02905119, -0.0006469622, 0.007898633, 0.09749723, -0.018853178, 0.019953946, 0.04994666, 0.02028865, 0.0094500445, -0.01038778, -0.02061057, -0.057090145, -0.003585003, 0.011452469, -0.05488647, -0.012916815, -0.005546279, -0.02436761, 0.0122234, -0.021649959, 0.012867827, -0.010473484, -0.019195965, -0.010870602, -0.044850674, 0.006287362, -0.026296316, 0.00039936657, 0.005192114, 0.015225565, -0.026449801, -0.019650705, -0.0027849763, -0.021247173, 0.021792322, 0.019227054, -0.0026935416, 0.015993213, -0.0154273715, 0.055392396, 0.006272421, 0.029723858, -0.030986901, -0.010381361, -0.00693142, 0.015882863, -0.043842375, 0.005712274, 0.037688617, 0.040173765, 0.069941, -9.0278745e-06, 0.017929543, -0.031960513, 0.030542372, -0.037162557, -0.027733864, -0.06739511, -0.036455087, -0.0024467784, -0.00069756265, 0.0647541, 0.0056464034, 0.008565647, -0.021407902, -0.039012127, 0.009042806, 0.059089914, -0.015314283, 0.047468454, 0.0467607, 0.07472078, 0.08624531, -0.00086820417, 0.0055072033, 0.045182046, -0.05155084, 0.017216386, 0.00651919, -0.0361654, 0.014263869, -0.019420374, -0.01828306, -0.023049807, 0.019316865, -0.024549117, 0.011808901, -0.022063993, 0.080597006, -0.024519194, -0.014161753, 0.02363851, -0.022520943, 0.021592546, 0.027026549, 0.04048286, -0.009430193, -0.068542376, -0.054683883, 0.015204016, 0.04702578, 0.0109518105, -0.018308196, -0.008895179, -0.028598918, -0.048651673, -0.0065117553, -0.0060110483, 0.031477638, 0.04925819, -0.022657719, 0.032201037, -0.028484398, 0.023292948, -0.040876936, -0.042882785, 0.06402289, -0.023073541, -0.00047751074, -0.11543645, -0.01734886, 0.05384718, -0.07837056]\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"\"\"What's new in iphone 16?\"\"\"\n",
    "user_prompt_embeddings = get_embedding(system_prompt)\n",
    "print(user_prompt_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_embeddings = [get_embedding(x) for x in data]\n",
    "data_embeddings_reshaped = np.array(data_embeddings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt_embeddings_reshaped = np.array(user_prompt_embeddings).reshape(1, -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe iPhone 16 introduces several exciting updates, making it one of Apple\\'s most advanced smartphones to date. It features a larger 6.1-inch display for the base model and a 6.7-inch screen for the iPhone 16 Plus, with thinner bezels and a more durable Ceramic Shield. The iPhone 16 Pro and Pro Max boast even larger displays, measuring 6.3 and 6.9 inches respectively, offering the thinnest bezels seen on any Apple product so far.\\n\\nPowered by the new A18 chip (A18 Pro for the Pro models), these phones deliver significant performance improvements, with enhanced neural engine capabilities, faster GPU for gaming, and machine learning tasks. The camera systems are also upgraded, with the base iPhone 16 sporting a dual-camera setup with a 48MP main sensor. The Pro models offer a 48MP Ultra Wide and 5x telephoto camera, enhanced by Apple’s \"Camera Control\" button for more flexible photography options.\\n\\nApple also introduced advanced audio features like \"Audio Mix,\" which uses machine learning to separate background sounds from speech, allowing for more refined audio capture during video recording. Battery life has been extended, especially in the iPhone 16 Pro Max, which is claimed to have the longest-lasting battery of any iPhone \\n9TO5MAC\\n\\nAPPLEMAGAZINE\\n.\\n\\nAdditionally, Apple has switched to USB-C for faster charging and data transfer, and the Pro models now support up to 2x faster video encoding. The starting prices remain consistent with previous generations, with the iPhone 16 starting at $799, while the Pro models start at $999\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_similarity_scores = cosine_similarity(user_prompt_embeddings_reshaped, data_embeddings_reshaped)\n",
    "\n",
    "closest_entry_index = data_similarity_scores.argmax()\n",
    "\n",
    "closest_entry = data[closest_entry_index]\n",
    "closest_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt_with_data = f\"\"\"\n",
    "{closest_entry}\n",
    "\n",
    "{user_prompt}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The iPhone 16 boasts several key updates:\n",
      "\n",
      "* **Larger Displays & Thinner Bezels:**  Larger screen sizes across all models (6.1-inch and 6.7-inch for base models, 6.3-inch and 6.9-inch for Pro models) with the thinnest bezels yet on an Apple product.  The screens also utilize a more durable Ceramic Shield.\n",
      "\n",
      "* **Improved Performance:** The A18 chip (A18 Pro for Pro models) offers significant performance boosts, including enhanced neural engine, faster GPU, and improved machine learning capabilities.\n",
      "\n",
      "* **Enhanced Camera Systems:**  The base model features a 48MP main sensor, while the Pro models include a 48MP Ultra Wide and a 5x telephoto camera, along with a new \"Camera Control\" button for improved photographic flexibility.\n",
      "\n",
      "* **Advanced Audio Features:** \"Audio Mix\" uses machine learning to separate background noise from speech during video recording.\n",
      "\n",
      "* **Extended Battery Life:**  Significantly improved battery life, especially in the iPhone 16 Pro Max, which boasts the longest battery life of any iPhone.\n",
      "\n",
      "* **USB-C Port:**  A switch to USB-C for faster charging and data transfer speeds.  Pro models also get up to 2x faster video encoding.\n",
      "\n",
      "* **Consistent Pricing:** Starting prices remain similar to previous generations ($799 for the base model, $999 for Pro models).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [{'text': user_prompt_with_data}]  \n",
    "response = get_chatbot_response(messages)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
